{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization Lab\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this lab, we'll gain experience detecting and dealing with a ANN model that is overfitting using various regularization and hyperparameter tuning techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this lab, we'll work with a large dataset of customer complaints to a bank, with the goal of predicting what product the customer is complaining about based on the text of their complaint.  There are 7 different possible products that we can predict, making this a multi-class classification task. \n",
    "\n",
    "\n",
    "#### Preprocessing our Data Set\n",
    "We'll start by preprocessing our dataset by tokenizing the complaints and limiting the number of words we consider to reduce dimensionality. \n",
    "\n",
    "#### Building our Tuning our Model\n",
    "Once we have preprocessed our data set, we'll build a model and explore the various ways that we can reduce overfitting using the following strategies:\n",
    "- Early stopping to minimize the discrepancy between train and test accuracy.\n",
    "- L1 and L2 regularization.\n",
    "- Dropout regularization.\n",
    "- Using more data.\n",
    "\n",
    "\n",
    "**_Let's Get Started!_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Bank Complaints Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the libraries and take a sample\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cell below, import our data into a DataFrame.  The data is currently stored in `Bank_complaints.csv`.\n",
    "Then, `.describe()` the dataset to get a feel for what we're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>59724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am filing this complaint because Experian ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11404</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Product                       Consumer complaint narrative\n",
       "count          60000                                              60000\n",
       "unique             7                                              59724\n",
       "top     Student loan  I am filing this complaint because Experian ha...\n",
       "freq           11404                                                 26"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed things up during the development process (and also to give us the ability to see how adding more data affects our model performance), we're going to work with a sample of our dataset rather than the whole thing.  The entire dataset consists of 60,000 rows--we're going to build a model using only 10,000 items randomly sampled from this.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Get a random sample of `10000` items from our dataset (HINT: use the `df` object's `.sample()` method to make this easy)\n",
    "* Reset the indexes on these samples to `range(10000)`, so that the indices for our rows are sequential and make sense.\n",
    "* Store our labels, which are found in `\"Product\"`, in a different variable.\n",
    "* Store the data, found in `\"Consumer complaint narrative`, in the variable `complaints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenizing the Complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only keep 2,000 most common words and use one-hot encoding to quickly vectorize our dataset from text into a format that a neural network can work with. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Create a `Tokenizer()` object, and set the `num_words` parameter to `2000`.\n",
    "* Call the tokenizer object's `fit_on_texts()` method and pass in our `complaints` variable we created above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create some text sequences by calling the `tokenizer` object's `.texts_to_sequences()` method and feeding in our `complaints` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(complaints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll convert our text data from text to a vectorized matrix.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Call the `tokenizer` object's `.texts_to_matrix` method, passing in our `complaints` variable, as well as setting the `mode` parameter equal to `'binary'`.\n",
    "* Store the tokenizer's `.word_index` in the appropriate variable.\n",
    "* Check the `np.shape()` of our `one_hot_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results) # Expected Results (10000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One-hot Encoding of the Products Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized and encoded our text data, we still need to one-hot encode our label data.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "\n",
    "* Create a `LabelEncoder` object, which can found inside the `preprocessing` module.\n",
    "* `fit` the label encoder we just created to `product`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what classes our label encoder found.  Run the cell below to examine a list of classes that `product` contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank account or service',\n",
       " 'Checking or savings account',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Credit reporting',\n",
       " 'Mortgage',\n",
       " 'Student loan']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to transform `product` into a numeric vector.  \n",
    "\n",
    "In the cell below, use the label encoder's `.transform` method on `product` to create an integer encoded version of our labels. \n",
    "\n",
    "Then, access `product_cat` to see an example of how the labels are now encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat = le.transform(product) \n",
    "product_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go from integer encoding to one-hot encoding.  Use the `to_categorical` method from keras to do this easily in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the shape of our one-hot encoded labels to make sure everything worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(product_onehot) # Expected Output: (10000, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train - test split\n",
    "\n",
    "Now, we'll split our data into training and testing sets.  \n",
    "\n",
    "\n",
    "We'll accomplish this by generating a random list of 1500 different indices between 1 and 10000.  Then, we'll slice these rows and store them as our test set, and delete them from the training set (it's very important to remember to remove them from the training set!)\n",
    "\n",
    "Run the cell below to create a set of random indices for our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = random.sample(range(1,10000), 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "\n",
    "* Slice the `test_index` rows from `one_hot_results` and store them in `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = one_hot_results[test_index]\n",
    "\n",
    "# This line returns a version of our one_hot_results that has every item with an index in test_index removed\n",
    "train = np.delete(one_hot_results, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll need to repeat the splitting process on our labels, making sure that we use the same indices we used to split our data. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Slice `test_index` from `product_onehot`\n",
    "* Use `np.delete` to remove `test_index` items from `product_onehot` (the syntax is exactly the same above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the shape everything we just did to make sure that the dimensions match up.  \n",
    "\n",
    "In the cell below, use `np.shape` to check the shape of:\n",
    "\n",
    "* `label_test`\n",
    "* `label_train`\n",
    "* `test`\n",
    "* `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 7)\n",
      "(8500, 7)\n",
      "(1500, 2000)\n",
      "(8500, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(label_test)) # Expected Output: (1500, 7)\n",
    "print(np.shape(label_train)) # Expected Output: (8500, 7)\n",
    "print(np.shape(test)) # Expected Output: (1500, 2000)\n",
    "print(np.shape(train)) # Expected Output: (8500, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`.\n",
    "\n",
    "Run the cell below to create our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = train[:1000]\n",
    "train_final = train[1000:]\n",
    "label_val = label_train[:1000]\n",
    "label_train_final = label_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating, compiling and running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Sequential` from the appropriate module in keras.\n",
    "* Import `Dense` from the appropriate module in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a model with the following specifications in the cell below:\n",
    "\n",
    "* An input layer of shape `(2000,)`\n",
    "* Hidden layer 1: Dense, 50 neurons, relu activation \n",
    "* Hidden layer 2: Dense, 25 neurons, relu activation\n",
    "* Output layer: Dense, 7 neurons, softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, `compile` the model with the following settings:\n",
    "\n",
    "* Optimizer is `\"SGD\"`\n",
    "* Loss is `'categorical_crossentropy'`\n",
    "* metrics is `['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Train the model for 120 epochs in mini-batches of 256 samples. Also pass in `(val, label_val)` to the `validation_data` parameter, so that we see how our model does on the test set after every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.9505 - accuracy: 0.1592 - val_loss: 1.9381 - val_accuracy: 0.1670\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9261 - accuracy: 0.1917 - val_loss: 1.9169 - val_accuracy: 0.2000\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9044 - accuracy: 0.2293 - val_loss: 1.8970 - val_accuracy: 0.2480\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8819 - accuracy: 0.2632 - val_loss: 1.8762 - val_accuracy: 0.2680\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8575 - accuracy: 0.2924 - val_loss: 1.8524 - val_accuracy: 0.2900\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8286 - accuracy: 0.3167 - val_loss: 1.8241 - val_accuracy: 0.3150\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7959 - accuracy: 0.3337 - val_loss: 1.7933 - val_accuracy: 0.3270\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7591 - accuracy: 0.3487 - val_loss: 1.7588 - val_accuracy: 0.3380\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7192 - accuracy: 0.3668 - val_loss: 1.7221 - val_accuracy: 0.3440\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6771 - accuracy: 0.3797 - val_loss: 1.6810 - val_accuracy: 0.3660\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6336 - accuracy: 0.3975 - val_loss: 1.6406 - val_accuracy: 0.3820\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5891 - accuracy: 0.4127 - val_loss: 1.5981 - val_accuracy: 0.4030\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5450 - accuracy: 0.4336 - val_loss: 1.5585 - val_accuracy: 0.4140\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5007 - accuracy: 0.4512 - val_loss: 1.5181 - val_accuracy: 0.4320\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4568 - accuracy: 0.4751 - val_loss: 1.4753 - val_accuracy: 0.4640\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4137 - accuracy: 0.4972 - val_loss: 1.4386 - val_accuracy: 0.4910\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3709 - accuracy: 0.5264 - val_loss: 1.4018 - val_accuracy: 0.5100\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3287 - accuracy: 0.5544 - val_loss: 1.3592 - val_accuracy: 0.5270\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2873 - accuracy: 0.5755 - val_loss: 1.3219 - val_accuracy: 0.5550\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.2471 - accuracy: 0.5981 - val_loss: 1.2858 - val_accuracy: 0.5750\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2081 - accuracy: 0.6172 - val_loss: 1.2464 - val_accuracy: 0.5910\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1702 - accuracy: 0.6313 - val_loss: 1.2158 - val_accuracy: 0.6010\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1339 - accuracy: 0.6457 - val_loss: 1.1827 - val_accuracy: 0.6130\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0988 - accuracy: 0.6587 - val_loss: 1.1497 - val_accuracy: 0.6230\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0659 - accuracy: 0.6675 - val_loss: 1.1191 - val_accuracy: 0.6310\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0339 - accuracy: 0.6789 - val_loss: 1.0911 - val_accuracy: 0.6370\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0039 - accuracy: 0.6881 - val_loss: 1.0681 - val_accuracy: 0.6350\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9753 - accuracy: 0.6960 - val_loss: 1.0420 - val_accuracy: 0.6460\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9488 - accuracy: 0.7027 - val_loss: 1.0156 - val_accuracy: 0.6490\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9232 - accuracy: 0.7105 - val_loss: 0.9942 - val_accuracy: 0.6670\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8992 - accuracy: 0.7157 - val_loss: 0.9765 - val_accuracy: 0.6630\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8769 - accuracy: 0.7213 - val_loss: 0.9541 - val_accuracy: 0.6740\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8563 - accuracy: 0.7279 - val_loss: 0.9393 - val_accuracy: 0.6830\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8361 - accuracy: 0.7324 - val_loss: 0.9194 - val_accuracy: 0.6820\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8175 - accuracy: 0.7357 - val_loss: 0.9054 - val_accuracy: 0.6860\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8000 - accuracy: 0.7413 - val_loss: 0.8974 - val_accuracy: 0.6850\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7833 - accuracy: 0.7449 - val_loss: 0.8789 - val_accuracy: 0.6980\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7675 - accuracy: 0.7501 - val_loss: 0.8679 - val_accuracy: 0.6950\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7525 - accuracy: 0.7535 - val_loss: 0.8545 - val_accuracy: 0.6920\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7391 - accuracy: 0.7584 - val_loss: 0.8462 - val_accuracy: 0.6950\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7255 - accuracy: 0.7631 - val_loss: 0.8333 - val_accuracy: 0.7020\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7130 - accuracy: 0.7657 - val_loss: 0.8235 - val_accuracy: 0.7080\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7011 - accuracy: 0.7695 - val_loss: 0.8175 - val_accuracy: 0.7110\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.7725 - val_loss: 0.8096 - val_accuracy: 0.6970\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6791 - accuracy: 0.7756 - val_loss: 0.7998 - val_accuracy: 0.7150\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6687 - accuracy: 0.7748 - val_loss: 0.7972 - val_accuracy: 0.7110\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6592 - accuracy: 0.7793 - val_loss: 0.7884 - val_accuracy: 0.7160\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6497 - accuracy: 0.7844 - val_loss: 0.7797 - val_accuracy: 0.7150\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.7865 - val_loss: 0.7721 - val_accuracy: 0.7230\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6320 - accuracy: 0.7877 - val_loss: 0.7700 - val_accuracy: 0.7230\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6236 - accuracy: 0.7917 - val_loss: 0.7642 - val_accuracy: 0.7190\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6158 - accuracy: 0.7940 - val_loss: 0.7575 - val_accuracy: 0.7240\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6080 - accuracy: 0.7951 - val_loss: 0.7515 - val_accuracy: 0.7200\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6009 - accuracy: 0.7973 - val_loss: 0.7496 - val_accuracy: 0.7210\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5934 - accuracy: 0.8003 - val_loss: 0.7448 - val_accuracy: 0.7220\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5868 - accuracy: 0.8032 - val_loss: 0.7419 - val_accuracy: 0.7260\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5803 - accuracy: 0.8040 - val_loss: 0.7422 - val_accuracy: 0.7250\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5738 - accuracy: 0.8068 - val_loss: 0.7351 - val_accuracy: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5672 - accuracy: 0.8081 - val_loss: 0.7306 - val_accuracy: 0.7270\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5615 - accuracy: 0.8100 - val_loss: 0.7301 - val_accuracy: 0.7300\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5555 - accuracy: 0.8115 - val_loss: 0.7256 - val_accuracy: 0.7330\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5496 - accuracy: 0.8136 - val_loss: 0.7263 - val_accuracy: 0.7280\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5443 - accuracy: 0.8153 - val_loss: 0.7189 - val_accuracy: 0.7300\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5386 - accuracy: 0.8185 - val_loss: 0.7176 - val_accuracy: 0.7300\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5338 - accuracy: 0.8188 - val_loss: 0.7155 - val_accuracy: 0.7320\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5285 - accuracy: 0.8196 - val_loss: 0.7144 - val_accuracy: 0.7350\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.8231 - val_loss: 0.7161 - val_accuracy: 0.7400\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5187 - accuracy: 0.8233 - val_loss: 0.7159 - val_accuracy: 0.7320\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5141 - accuracy: 0.8249 - val_loss: 0.7139 - val_accuracy: 0.7360\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5098 - accuracy: 0.8268 - val_loss: 0.7051 - val_accuracy: 0.7380\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5052 - accuracy: 0.8280 - val_loss: 0.7066 - val_accuracy: 0.7380\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5004 - accuracy: 0.8296 - val_loss: 0.7038 - val_accuracy: 0.7390\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4959 - accuracy: 0.8311 - val_loss: 0.7007 - val_accuracy: 0.7410\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4919 - accuracy: 0.8321 - val_loss: 0.7046 - val_accuracy: 0.7410\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4875 - accuracy: 0.8348 - val_loss: 0.7002 - val_accuracy: 0.7440\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4836 - accuracy: 0.8368 - val_loss: 0.6980 - val_accuracy: 0.7390\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4794 - accuracy: 0.8372 - val_loss: 0.6968 - val_accuracy: 0.7420\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4753 - accuracy: 0.8408 - val_loss: 0.6947 - val_accuracy: 0.7380\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4716 - accuracy: 0.8408 - val_loss: 0.6921 - val_accuracy: 0.7440\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4678 - accuracy: 0.8415 - val_loss: 0.6963 - val_accuracy: 0.7380\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.8415 - val_loss: 0.6916 - val_accuracy: 0.7350\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.8463 - val_loss: 0.6920 - val_accuracy: 0.7390\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.4566 - accuracy: 0.8444 - val_loss: 0.6884 - val_accuracy: 0.7430\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.8469 - val_loss: 0.6878 - val_accuracy: 0.7430\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8484 - val_loss: 0.6916 - val_accuracy: 0.7460\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4455 - accuracy: 0.8496 - val_loss: 0.6915 - val_accuracy: 0.7460\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.8501 - val_loss: 0.6878 - val_accuracy: 0.7490\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4391 - accuracy: 0.8508 - val_loss: 0.6875 - val_accuracy: 0.7440\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4353 - accuracy: 0.8537 - val_loss: 0.6950 - val_accuracy: 0.7450\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8555 - val_loss: 0.6894 - val_accuracy: 0.7490\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4292 - accuracy: 0.8552 - val_loss: 0.6866 - val_accuracy: 0.7480\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4258 - accuracy: 0.8568 - val_loss: 0.6840 - val_accuracy: 0.7510\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.8591 - val_loss: 0.6853 - val_accuracy: 0.7440\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4198 - accuracy: 0.8585 - val_loss: 0.6854 - val_accuracy: 0.7420\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4166 - accuracy: 0.8595 - val_loss: 0.6839 - val_accuracy: 0.7470\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4135 - accuracy: 0.8612 - val_loss: 0.6872 - val_accuracy: 0.7460\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4102 - accuracy: 0.8620 - val_loss: 0.6867 - val_accuracy: 0.7440\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4073 - accuracy: 0.8652 - val_loss: 0.6870 - val_accuracy: 0.7440\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4046 - accuracy: 0.8639 - val_loss: 0.6901 - val_accuracy: 0.7450\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4018 - accuracy: 0.8668 - val_loss: 0.6811 - val_accuracy: 0.7540\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3986 - accuracy: 0.8677 - val_loss: 0.6818 - val_accuracy: 0.7530\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3953 - accuracy: 0.8696 - val_loss: 0.6828 - val_accuracy: 0.7470\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8721 - val_loss: 0.6833 - val_accuracy: 0.7530\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8707 - val_loss: 0.6803 - val_accuracy: 0.7500\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3873 - accuracy: 0.8724 - val_loss: 0.6842 - val_accuracy: 0.7570\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3848 - accuracy: 0.8743 - val_loss: 0.6803 - val_accuracy: 0.7540\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3817 - accuracy: 0.8747 - val_loss: 0.6831 - val_accuracy: 0.7500\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8767 - val_loss: 0.6792 - val_accuracy: 0.7550\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3768 - accuracy: 0.8777 - val_loss: 0.6817 - val_accuracy: 0.7500\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8783 - val_loss: 0.6881 - val_accuracy: 0.7550\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8795 - val_loss: 0.6863 - val_accuracy: 0.7540\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3690 - accuracy: 0.8793 - val_loss: 0.6829 - val_accuracy: 0.7530\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3660 - accuracy: 0.8821 - val_loss: 0.6824 - val_accuracy: 0.7570\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8829 - val_loss: 0.6834 - val_accuracy: 0.7500\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3614 - accuracy: 0.8837 - val_loss: 0.6841 - val_accuracy: 0.7590\n",
      "Epoch 116/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3582 - accuracy: 0.8855 - val_loss: 0.6836 - val_accuracy: 0.7480\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3564 - accuracy: 0.8873 - val_loss: 0.6834 - val_accuracy: 0.7490\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3543 - accuracy: 0.8868 - val_loss: 0.6912 - val_accuracy: 0.7630\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3520 - accuracy: 0.8881 - val_loss: 0.6938 - val_accuracy: 0.7580\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3495 - accuracy: 0.8884 - val_loss: 0.6870 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store the model's `.history` inside of `model_val_dict`\n",
    "* Check what `keys()` this dictionary contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the final results on the training and testing sets using `model.evaluate()` on `train_final` and `label_train_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 430us/step - loss: 0.3479 - accuracy: 0.8887\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use this function to get the results on our testing set.  Call the function again, but this time on `test` and `label_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 447us/step - loss: 0.6851 - accuracy: 0.7567\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the contents of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3478814959526062, 0.8886666893959045]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.33576024494171142, 0.89600000000000002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6851280331611633, 0.7566666603088379]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.72006658554077152, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. Let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy.\n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g.', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! \n",
    "\n",
    "Run the cell below to visualize a plot of our training and validation accuracy>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-3f4bab085b80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0macc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_val_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_val_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early stopping\n",
    "\n",
    "Now that we know that the model starts to overfit around epoch 60, we can just retrain the model from scratch, but this time only up to 60 epochs! This will help us with our overfitting problem.  This method is called **_Early Stopping_**.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the exact model we did above. \n",
    "* Compile the model with the exact same hyperparameters.\n",
    "* Fit the model with the exact same hyperparameters, with the exception of `epochs`.  This time, set epochs to `60` instead of `120`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9528 - accuracy: 0.1436 - val_loss: 1.9540 - val_accuracy: 0.1410\n",
      "Epoch 2/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9325 - accuracy: 0.1765 - val_loss: 1.9348 - val_accuracy: 0.1680\n",
      "Epoch 3/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9122 - accuracy: 0.2052 - val_loss: 1.9146 - val_accuracy: 0.1880\n",
      "Epoch 4/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8894 - accuracy: 0.2389 - val_loss: 1.8919 - val_accuracy: 0.2150\n",
      "Epoch 5/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8631 - accuracy: 0.2637 - val_loss: 1.8647 - val_accuracy: 0.2330\n",
      "Epoch 6/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8329 - accuracy: 0.2896 - val_loss: 1.8345 - val_accuracy: 0.2580\n",
      "Epoch 7/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7993 - accuracy: 0.3100 - val_loss: 1.8018 - val_accuracy: 0.2850\n",
      "Epoch 8/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7621 - accuracy: 0.3272 - val_loss: 1.7657 - val_accuracy: 0.3080\n",
      "Epoch 9/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7215 - accuracy: 0.3472 - val_loss: 1.7272 - val_accuracy: 0.3290\n",
      "Epoch 10/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6787 - accuracy: 0.3684 - val_loss: 1.6866 - val_accuracy: 0.3470\n",
      "Epoch 11/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6338 - accuracy: 0.3883 - val_loss: 1.6447 - val_accuracy: 0.3680\n",
      "Epoch 12/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5874 - accuracy: 0.4160 - val_loss: 1.6025 - val_accuracy: 0.3940\n",
      "Epoch 13/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5401 - accuracy: 0.4479 - val_loss: 1.5573 - val_accuracy: 0.4330\n",
      "Epoch 14/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4923 - accuracy: 0.4759 - val_loss: 1.5143 - val_accuracy: 0.4530\n",
      "Epoch 15/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4449 - accuracy: 0.4975 - val_loss: 1.4703 - val_accuracy: 0.4820\n",
      "Epoch 16/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3977 - accuracy: 0.5227 - val_loss: 1.4302 - val_accuracy: 0.4910\n",
      "Epoch 17/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3524 - accuracy: 0.5435 - val_loss: 1.3885 - val_accuracy: 0.5110\n",
      "Epoch 18/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3085 - accuracy: 0.5660 - val_loss: 1.3469 - val_accuracy: 0.5240\n",
      "Epoch 19/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2664 - accuracy: 0.5807 - val_loss: 1.3114 - val_accuracy: 0.5310\n",
      "Epoch 20/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2264 - accuracy: 0.6007 - val_loss: 1.2732 - val_accuracy: 0.5490\n",
      "Epoch 21/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1881 - accuracy: 0.6149 - val_loss: 1.2381 - val_accuracy: 0.5630\n",
      "Epoch 22/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1522 - accuracy: 0.6285 - val_loss: 1.2049 - val_accuracy: 0.5780\n",
      "Epoch 23/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1182 - accuracy: 0.6397 - val_loss: 1.1745 - val_accuracy: 0.5840\n",
      "Epoch 24/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0861 - accuracy: 0.6464 - val_loss: 1.1455 - val_accuracy: 0.5950\n",
      "Epoch 25/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0558 - accuracy: 0.6563 - val_loss: 1.1212 - val_accuracy: 0.5990\n",
      "Epoch 26/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0272 - accuracy: 0.6637 - val_loss: 1.0966 - val_accuracy: 0.6030\n",
      "Epoch 27/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0008 - accuracy: 0.6713 - val_loss: 1.0715 - val_accuracy: 0.6060\n",
      "Epoch 28/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9757 - accuracy: 0.6779 - val_loss: 1.0499 - val_accuracy: 0.6130\n",
      "Epoch 29/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9522 - accuracy: 0.6836 - val_loss: 1.0284 - val_accuracy: 0.6230\n",
      "Epoch 30/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9302 - accuracy: 0.6888 - val_loss: 1.0078 - val_accuracy: 0.6210\n",
      "Epoch 31/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9093 - accuracy: 0.6925 - val_loss: 0.9900 - val_accuracy: 0.6310\n",
      "Epoch 32/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8892 - accuracy: 0.6992 - val_loss: 0.9779 - val_accuracy: 0.6360\n",
      "Epoch 33/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8711 - accuracy: 0.7057 - val_loss: 0.9575 - val_accuracy: 0.6330\n",
      "Epoch 34/60\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8535 - accuracy: 0.7068 - val_loss: 0.9403 - val_accuracy: 0.6480\n",
      "Epoch 35/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8370 - accuracy: 0.7139 - val_loss: 0.9292 - val_accuracy: 0.6510\n",
      "Epoch 36/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8218 - accuracy: 0.7180 - val_loss: 0.9130 - val_accuracy: 0.6580\n",
      "Epoch 37/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8071 - accuracy: 0.7217 - val_loss: 0.9033 - val_accuracy: 0.6640\n",
      "Epoch 38/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7936 - accuracy: 0.7255 - val_loss: 0.8927 - val_accuracy: 0.6590\n",
      "Epoch 39/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7797 - accuracy: 0.7317 - val_loss: 0.8806 - val_accuracy: 0.6640\n",
      "Epoch 40/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7677 - accuracy: 0.7349 - val_loss: 0.8720 - val_accuracy: 0.6670\n",
      "Epoch 41/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7555 - accuracy: 0.7379 - val_loss: 0.8609 - val_accuracy: 0.6720\n",
      "Epoch 42/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7447 - accuracy: 0.7417 - val_loss: 0.8542 - val_accuracy: 0.6690\n",
      "Epoch 43/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7340 - accuracy: 0.7431 - val_loss: 0.8467 - val_accuracy: 0.6740\n",
      "Epoch 44/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7235 - accuracy: 0.7465 - val_loss: 0.8417 - val_accuracy: 0.6730\n",
      "Epoch 45/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7140 - accuracy: 0.7515 - val_loss: 0.8330 - val_accuracy: 0.6720\n",
      "Epoch 46/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7044 - accuracy: 0.7507 - val_loss: 0.8236 - val_accuracy: 0.6790\n",
      "Epoch 47/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6956 - accuracy: 0.7569 - val_loss: 0.8157 - val_accuracy: 0.6820\n",
      "Epoch 48/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.7591 - val_loss: 0.8143 - val_accuracy: 0.6770\n",
      "Epoch 49/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6785 - accuracy: 0.7617 - val_loss: 0.8076 - val_accuracy: 0.6830\n",
      "Epoch 50/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6700 - accuracy: 0.7640 - val_loss: 0.8048 - val_accuracy: 0.6810\n",
      "Epoch 51/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6627 - accuracy: 0.7681 - val_loss: 0.7952 - val_accuracy: 0.6900\n",
      "Epoch 52/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6549 - accuracy: 0.7701 - val_loss: 0.7893 - val_accuracy: 0.6910\n",
      "Epoch 53/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6474 - accuracy: 0.7735 - val_loss: 0.7862 - val_accuracy: 0.6920\n",
      "Epoch 54/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6404 - accuracy: 0.7761 - val_loss: 0.7832 - val_accuracy: 0.6930\n",
      "Epoch 55/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6337 - accuracy: 0.7769 - val_loss: 0.7763 - val_accuracy: 0.6980\n",
      "Epoch 56/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6264 - accuracy: 0.7800 - val_loss: 0.7752 - val_accuracy: 0.6960\n",
      "Epoch 57/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6203 - accuracy: 0.7799 - val_loss: 0.7693 - val_accuracy: 0.7000\n",
      "Epoch 58/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6141 - accuracy: 0.7841 - val_loss: 0.7688 - val_accuracy: 0.6950\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6078 - accuracy: 0.7849 - val_loss: 0.7681 - val_accuracy: 0.6890\n",
      "Epoch 60/60\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6023 - accuracy: 0.7863 - val_loss: 0.7624 - val_accuracy: 0.6970\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did before, get our results using `model.evaluate()` on the appropriate variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 434us/step - loss: 0.5967 - accuracy: 0.7889\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 468us/step - loss: 0.7377 - accuracy: 0.7167\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5967416167259216, 0.7889333367347717]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train  # Expected Output: [0.58606486314137773, 0.79826666669845581]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.737703263759613, 0.7166666388511658]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # [0.74768974288304646, 0.71333333365122475]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument `kernel_regulizers.l2` and adding a value for the regularization parameter lambda between parentheses.\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did before.\n",
    "* In our two hidden layers (but not our output layer), add in the parameter `kernel_regularizer=regularizers.l2(0.005)` to add L2 regularization to each hidden layer.  \n",
    "* Compile the model with the same hyperparameters as we did before. \n",
    "* Fit the model with the same hyperparameters as we did before, but this time for `120` epochs.\n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L2_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 2.6102 - accuracy: 0.1671 - val_loss: 2.5923 - val_accuracy: 0.1870\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.5833 - accuracy: 0.1981 - val_loss: 2.5726 - val_accuracy: 0.2100\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.5594 - accuracy: 0.2221 - val_loss: 2.5539 - val_accuracy: 0.2290\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.5358 - accuracy: 0.2437 - val_loss: 2.5333 - val_accuracy: 0.2370\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.5111 - accuracy: 0.2588 - val_loss: 2.5097 - val_accuracy: 0.2500\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.4828 - accuracy: 0.2725 - val_loss: 2.4815 - val_accuracy: 0.2670\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.4501 - accuracy: 0.2959 - val_loss: 2.4491 - val_accuracy: 0.2890\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.4127 - accuracy: 0.3227 - val_loss: 2.4112 - val_accuracy: 0.3250\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.3709 - accuracy: 0.3553 - val_loss: 2.3708 - val_accuracy: 0.3530\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.3251 - accuracy: 0.3887 - val_loss: 2.3272 - val_accuracy: 0.3870\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.2767 - accuracy: 0.4212 - val_loss: 2.2805 - val_accuracy: 0.4160\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.2257 - accuracy: 0.4556 - val_loss: 2.2347 - val_accuracy: 0.4390\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.1742 - accuracy: 0.4881 - val_loss: 2.1845 - val_accuracy: 0.4690\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.1213 - accuracy: 0.5171 - val_loss: 2.1350 - val_accuracy: 0.4920\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.0683 - accuracy: 0.5467 - val_loss: 2.0859 - val_accuracy: 0.5250\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.0159 - accuracy: 0.5715 - val_loss: 2.0369 - val_accuracy: 0.5570\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.9644 - accuracy: 0.5959 - val_loss: 1.9902 - val_accuracy: 0.5700\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9149 - accuracy: 0.6157 - val_loss: 1.9426 - val_accuracy: 0.5890\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8672 - accuracy: 0.6364 - val_loss: 1.8997 - val_accuracy: 0.5960\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8219 - accuracy: 0.6477 - val_loss: 1.8589 - val_accuracy: 0.6110\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.7785 - accuracy: 0.6600 - val_loss: 1.8195 - val_accuracy: 0.6330\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7376 - accuracy: 0.6720 - val_loss: 1.7859 - val_accuracy: 0.6340\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6994 - accuracy: 0.6852 - val_loss: 1.7479 - val_accuracy: 0.6570\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6637 - accuracy: 0.6929 - val_loss: 1.7158 - val_accuracy: 0.6580\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6300 - accuracy: 0.7013 - val_loss: 1.6876 - val_accuracy: 0.6640\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5981 - accuracy: 0.7075 - val_loss: 1.6584 - val_accuracy: 0.6740\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5686 - accuracy: 0.7164 - val_loss: 1.6316 - val_accuracy: 0.6750\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5407 - accuracy: 0.7252 - val_loss: 1.6076 - val_accuracy: 0.6860\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5142 - accuracy: 0.7308 - val_loss: 1.5848 - val_accuracy: 0.6850\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4900 - accuracy: 0.7339 - val_loss: 1.5626 - val_accuracy: 0.6870\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.4668 - accuracy: 0.7385 - val_loss: 1.5458 - val_accuracy: 0.6850\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4450 - accuracy: 0.7412 - val_loss: 1.5236 - val_accuracy: 0.6970\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4241 - accuracy: 0.7461 - val_loss: 1.5028 - val_accuracy: 0.7010\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4046 - accuracy: 0.7489 - val_loss: 1.4877 - val_accuracy: 0.7020\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3858 - accuracy: 0.7517 - val_loss: 1.4708 - val_accuracy: 0.7080\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3688 - accuracy: 0.7563 - val_loss: 1.4581 - val_accuracy: 0.7100\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3525 - accuracy: 0.7588 - val_loss: 1.4450 - val_accuracy: 0.7020\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3367 - accuracy: 0.7636 - val_loss: 1.4319 - val_accuracy: 0.7080\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3223 - accuracy: 0.7637 - val_loss: 1.4188 - val_accuracy: 0.7070\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3082 - accuracy: 0.7643 - val_loss: 1.4124 - val_accuracy: 0.7110\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.2951 - accuracy: 0.7695 - val_loss: 1.4000 - val_accuracy: 0.7070\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2822 - accuracy: 0.7712 - val_loss: 1.3877 - val_accuracy: 0.7080\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2696 - accuracy: 0.7745 - val_loss: 1.3782 - val_accuracy: 0.7150\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2585 - accuracy: 0.7771 - val_loss: 1.3701 - val_accuracy: 0.7150\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2476 - accuracy: 0.7783 - val_loss: 1.3641 - val_accuracy: 0.7140\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2366 - accuracy: 0.7823 - val_loss: 1.3566 - val_accuracy: 0.7150\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2266 - accuracy: 0.7848 - val_loss: 1.3481 - val_accuracy: 0.7170\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2168 - accuracy: 0.7851 - val_loss: 1.3413 - val_accuracy: 0.7200\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2068 - accuracy: 0.7872 - val_loss: 1.3381 - val_accuracy: 0.7180\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1979 - accuracy: 0.7901 - val_loss: 1.3269 - val_accuracy: 0.7210\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1888 - accuracy: 0.7913 - val_loss: 1.3219 - val_accuracy: 0.7200\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1802 - accuracy: 0.7949 - val_loss: 1.3185 - val_accuracy: 0.7220\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1724 - accuracy: 0.7955 - val_loss: 1.3105 - val_accuracy: 0.7220\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1640 - accuracy: 0.7975 - val_loss: 1.3029 - val_accuracy: 0.7230\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1560 - accuracy: 0.7988 - val_loss: 1.3005 - val_accuracy: 0.7220\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1480 - accuracy: 0.8048 - val_loss: 1.2939 - val_accuracy: 0.7270\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1405 - accuracy: 0.8039 - val_loss: 1.2915 - val_accuracy: 0.7240\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1332 - accuracy: 0.8036 - val_loss: 1.2854 - val_accuracy: 0.7320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1259 - accuracy: 0.8044 - val_loss: 1.2804 - val_accuracy: 0.7280\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1193 - accuracy: 0.8095 - val_loss: 1.2787 - val_accuracy: 0.7310\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1126 - accuracy: 0.8099 - val_loss: 1.2709 - val_accuracy: 0.7280\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1053 - accuracy: 0.8117 - val_loss: 1.2675 - val_accuracy: 0.7300\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0989 - accuracy: 0.8141 - val_loss: 1.2618 - val_accuracy: 0.7340\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0924 - accuracy: 0.8140 - val_loss: 1.2591 - val_accuracy: 0.7310\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0857 - accuracy: 0.8173 - val_loss: 1.2568 - val_accuracy: 0.7370\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0794 - accuracy: 0.8173 - val_loss: 1.2506 - val_accuracy: 0.7370\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0734 - accuracy: 0.8179 - val_loss: 1.2499 - val_accuracy: 0.7420\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0675 - accuracy: 0.8212 - val_loss: 1.2438 - val_accuracy: 0.7380\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0614 - accuracy: 0.8216 - val_loss: 1.2440 - val_accuracy: 0.7380\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0557 - accuracy: 0.8223 - val_loss: 1.2375 - val_accuracy: 0.7430\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0497 - accuracy: 0.8256 - val_loss: 1.2346 - val_accuracy: 0.7450\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0439 - accuracy: 0.8251 - val_loss: 1.2316 - val_accuracy: 0.7410\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0378 - accuracy: 0.8269 - val_loss: 1.2313 - val_accuracy: 0.7390\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0328 - accuracy: 0.8284 - val_loss: 1.2238 - val_accuracy: 0.7430\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0272 - accuracy: 0.8308 - val_loss: 1.2203 - val_accuracy: 0.7430\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0221 - accuracy: 0.8312 - val_loss: 1.2206 - val_accuracy: 0.7430\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0169 - accuracy: 0.8316 - val_loss: 1.2154 - val_accuracy: 0.7430\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0113 - accuracy: 0.8340 - val_loss: 1.2126 - val_accuracy: 0.7450\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0065 - accuracy: 0.8343 - val_loss: 1.2125 - val_accuracy: 0.7480\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0011 - accuracy: 0.8351 - val_loss: 1.2111 - val_accuracy: 0.7390\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9963 - accuracy: 0.8391 - val_loss: 1.2057 - val_accuracy: 0.7440\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9908 - accuracy: 0.8399 - val_loss: 1.2026 - val_accuracy: 0.7470\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9863 - accuracy: 0.8392 - val_loss: 1.2001 - val_accuracy: 0.7500\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9814 - accuracy: 0.8413 - val_loss: 1.1961 - val_accuracy: 0.7490\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9760 - accuracy: 0.8443 - val_loss: 1.1940 - val_accuracy: 0.7440\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9718 - accuracy: 0.8432 - val_loss: 1.1904 - val_accuracy: 0.7430\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9668 - accuracy: 0.8439 - val_loss: 1.1881 - val_accuracy: 0.7480\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9623 - accuracy: 0.8467 - val_loss: 1.1870 - val_accuracy: 0.7530\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9579 - accuracy: 0.8469 - val_loss: 1.1853 - val_accuracy: 0.7460\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9532 - accuracy: 0.8485 - val_loss: 1.1819 - val_accuracy: 0.7500\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9491 - accuracy: 0.8497 - val_loss: 1.1859 - val_accuracy: 0.7430\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9450 - accuracy: 0.8507 - val_loss: 1.1773 - val_accuracy: 0.7480\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9397 - accuracy: 0.8523 - val_loss: 1.1773 - val_accuracy: 0.7500\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9354 - accuracy: 0.8525 - val_loss: 1.1727 - val_accuracy: 0.7480\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9315 - accuracy: 0.8533 - val_loss: 1.1714 - val_accuracy: 0.7460\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9271 - accuracy: 0.8556 - val_loss: 1.1707 - val_accuracy: 0.7470\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9229 - accuracy: 0.8568 - val_loss: 1.1678 - val_accuracy: 0.7510\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9187 - accuracy: 0.8587 - val_loss: 1.1663 - val_accuracy: 0.7490\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9145 - accuracy: 0.8592 - val_loss: 1.1624 - val_accuracy: 0.7450\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9102 - accuracy: 0.8600 - val_loss: 1.1636 - val_accuracy: 0.7480\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9065 - accuracy: 0.8625 - val_loss: 1.1599 - val_accuracy: 0.7500\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9025 - accuracy: 0.8608 - val_loss: 1.1558 - val_accuracy: 0.7460\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8983 - accuracy: 0.8627 - val_loss: 1.1542 - val_accuracy: 0.7510\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8948 - accuracy: 0.8641 - val_loss: 1.1522 - val_accuracy: 0.7510\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8907 - accuracy: 0.8641 - val_loss: 1.1523 - val_accuracy: 0.7520\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8870 - accuracy: 0.8655 - val_loss: 1.1515 - val_accuracy: 0.7470\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8833 - accuracy: 0.8675 - val_loss: 1.1489 - val_accuracy: 0.7510\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8788 - accuracy: 0.8672 - val_loss: 1.1454 - val_accuracy: 0.7490\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8757 - accuracy: 0.8700 - val_loss: 1.1441 - val_accuracy: 0.7520\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8717 - accuracy: 0.8691 - val_loss: 1.1425 - val_accuracy: 0.7510\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8683 - accuracy: 0.8721 - val_loss: 1.1399 - val_accuracy: 0.7510\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8642 - accuracy: 0.8708 - val_loss: 1.1407 - val_accuracy: 0.7520\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8608 - accuracy: 0.8733 - val_loss: 1.1378 - val_accuracy: 0.7510\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8568 - accuracy: 0.8757 - val_loss: 1.1352 - val_accuracy: 0.7520\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8534 - accuracy: 0.8747 - val_loss: 1.1327 - val_accuracy: 0.7510\n",
      "Epoch 116/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8501 - accuracy: 0.8768 - val_loss: 1.1321 - val_accuracy: 0.7510\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8468 - accuracy: 0.8760 - val_loss: 1.1321 - val_accuracy: 0.7520\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8433 - accuracy: 0.8753 - val_loss: 1.1302 - val_accuracy: 0.7540\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8399 - accuracy: 0.8779 - val_loss: 1.1303 - val_accuracy: 0.7490\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8364 - accuracy: 0.8801 - val_loss: 1.1270 - val_accuracy: 0.7520\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how regularization has affected our model results.  \n",
    "\n",
    "Run the cell below to get the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs).\n",
    "\n",
    "Run the cell below to visualize our training and validation accuracy both with and without L2 regularization, so that we can compare them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-3a3d5a567663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0macc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL2_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL2_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_val_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?\n",
    "\n",
    "In the cell below: \n",
    "\n",
    "* Recreate the same model we did above, but this time, set the `kernel_regularizer` to `regularizers.l1(0.005)` inside both hidden layers. \n",
    "* Compile and fit the model exactly as we did for our L2 Regularization experiment (`120` epochs) \n",
    "* Store the fitted model that the `.fit` call returns inside a variable called `L1_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 16.0118 - accuracy: 0.1284 - val_loss: 15.5945 - val_accuracy: 0.1560\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 15.2376 - accuracy: 0.1816 - val_loss: 14.8429 - val_accuracy: 0.1890\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 14.4962 - accuracy: 0.2233 - val_loss: 14.1169 - val_accuracy: 0.2270\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 13.7777 - accuracy: 0.2464 - val_loss: 13.4104 - val_accuracy: 0.2540\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 13.0776 - accuracy: 0.2672 - val_loss: 12.7215 - val_accuracy: 0.2870\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 12.3948 - accuracy: 0.2867 - val_loss: 12.0495 - val_accuracy: 0.3000\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 11.7291 - accuracy: 0.3116 - val_loss: 11.3956 - val_accuracy: 0.3100\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 11.0813 - accuracy: 0.3303 - val_loss: 10.7607 - val_accuracy: 0.3270\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.4536 - accuracy: 0.3439 - val_loss: 10.1435 - val_accuracy: 0.3470\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 9.8482 - accuracy: 0.3557 - val_loss: 9.5509 - val_accuracy: 0.3610\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 9.2662 - accuracy: 0.3644 - val_loss: 8.9837 - val_accuracy: 0.3740\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 8.7078 - accuracy: 0.3796 - val_loss: 8.4384 - val_accuracy: 0.3940\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 8.1736 - accuracy: 0.3917 - val_loss: 7.9180 - val_accuracy: 0.4110\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 7.6632 - accuracy: 0.4084 - val_loss: 7.4204 - val_accuracy: 0.4210\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 7.1774 - accuracy: 0.4287 - val_loss: 6.9499 - val_accuracy: 0.4390\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 6.7156 - accuracy: 0.4500 - val_loss: 6.4999 - val_accuracy: 0.4520\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 6.2771 - accuracy: 0.4691 - val_loss: 6.0754 - val_accuracy: 0.4540\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 5.8622 - accuracy: 0.4877 - val_loss: 5.6750 - val_accuracy: 0.4690\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 5.4709 - accuracy: 0.5048 - val_loss: 5.2941 - val_accuracy: 0.4930\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 5.1030 - accuracy: 0.5175 - val_loss: 4.9424 - val_accuracy: 0.5030\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 4.7575 - accuracy: 0.5311 - val_loss: 4.6076 - val_accuracy: 0.5100\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 4.4345 - accuracy: 0.5484 - val_loss: 4.2933 - val_accuracy: 0.5360\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 4.1344 - accuracy: 0.5599 - val_loss: 4.0078 - val_accuracy: 0.5320\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 3.8554 - accuracy: 0.5672 - val_loss: 3.7423 - val_accuracy: 0.5410\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 3.5988 - accuracy: 0.5787 - val_loss: 3.4931 - val_accuracy: 0.5730\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 3.3640 - accuracy: 0.5873 - val_loss: 3.2709 - val_accuracy: 0.5700\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 3.1500 - accuracy: 0.5883 - val_loss: 3.0675 - val_accuracy: 0.5760\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.9568 - accuracy: 0.5983 - val_loss: 2.8858 - val_accuracy: 0.5890\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.7849 - accuracy: 0.6043 - val_loss: 2.7245 - val_accuracy: 0.5910\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.6335 - accuracy: 0.6041 - val_loss: 2.5828 - val_accuracy: 0.5890\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.5022 - accuracy: 0.6103 - val_loss: 2.4608 - val_accuracy: 0.5980\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.3901 - accuracy: 0.6143 - val_loss: 2.3596 - val_accuracy: 0.5980\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.2971 - accuracy: 0.6135 - val_loss: 2.2739 - val_accuracy: 0.6060\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.2217 - accuracy: 0.6192 - val_loss: 2.2107 - val_accuracy: 0.6220\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.1628 - accuracy: 0.6243 - val_loss: 2.1562 - val_accuracy: 0.6160\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.1193 - accuracy: 0.6281 - val_loss: 2.1207 - val_accuracy: 0.6150\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.0879 - accuracy: 0.6353 - val_loss: 2.0921 - val_accuracy: 0.6260\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.0625 - accuracy: 0.6327 - val_loss: 2.0707 - val_accuracy: 0.6250\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.0403 - accuracy: 0.6393 - val_loss: 2.0476 - val_accuracy: 0.6380\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.0200 - accuracy: 0.6380 - val_loss: 2.0309 - val_accuracy: 0.6300\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 2.0008 - accuracy: 0.6407 - val_loss: 2.0113 - val_accuracy: 0.6350\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9829 - accuracy: 0.6443 - val_loss: 1.9934 - val_accuracy: 0.6340\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.9654 - accuracy: 0.6499 - val_loss: 1.9762 - val_accuracy: 0.6490\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9481 - accuracy: 0.6540 - val_loss: 1.9608 - val_accuracy: 0.6420\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9321 - accuracy: 0.6596 - val_loss: 1.9429 - val_accuracy: 0.6480\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9162 - accuracy: 0.6581 - val_loss: 1.9292 - val_accuracy: 0.6540\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9010 - accuracy: 0.6632 - val_loss: 1.9149 - val_accuracy: 0.6460\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8859 - accuracy: 0.6649 - val_loss: 1.8990 - val_accuracy: 0.6600\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8715 - accuracy: 0.6672 - val_loss: 1.8851 - val_accuracy: 0.6620\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8569 - accuracy: 0.6695 - val_loss: 1.8759 - val_accuracy: 0.6460\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8440 - accuracy: 0.6725 - val_loss: 1.8574 - val_accuracy: 0.6630\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8302 - accuracy: 0.6744 - val_loss: 1.8456 - val_accuracy: 0.6670\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8168 - accuracy: 0.6759 - val_loss: 1.8329 - val_accuracy: 0.6650\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.8044 - accuracy: 0.6769 - val_loss: 1.8223 - val_accuracy: 0.6660\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7921 - accuracy: 0.6785 - val_loss: 1.8089 - val_accuracy: 0.6710\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7799 - accuracy: 0.6825 - val_loss: 1.7981 - val_accuracy: 0.6740\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7682 - accuracy: 0.6823 - val_loss: 1.7901 - val_accuracy: 0.6760\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 4ms/step - loss: 1.7573 - accuracy: 0.6825 - val_loss: 1.7771 - val_accuracy: 0.6760\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7456 - accuracy: 0.6864 - val_loss: 1.7667 - val_accuracy: 0.6780\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7345 - accuracy: 0.6865 - val_loss: 1.7584 - val_accuracy: 0.6800\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7240 - accuracy: 0.6860 - val_loss: 1.7479 - val_accuracy: 0.6760\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7134 - accuracy: 0.6860 - val_loss: 1.7349 - val_accuracy: 0.6810\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7028 - accuracy: 0.6907 - val_loss: 1.7282 - val_accuracy: 0.6810\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6934 - accuracy: 0.6889 - val_loss: 1.7136 - val_accuracy: 0.6860\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6830 - accuracy: 0.6937 - val_loss: 1.7059 - val_accuracy: 0.6860\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6735 - accuracy: 0.6928 - val_loss: 1.6989 - val_accuracy: 0.6870\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6636 - accuracy: 0.6945 - val_loss: 1.6882 - val_accuracy: 0.6870\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6543 - accuracy: 0.6947 - val_loss: 1.6793 - val_accuracy: 0.6900\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6450 - accuracy: 0.6949 - val_loss: 1.6691 - val_accuracy: 0.6920\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6356 - accuracy: 0.6968 - val_loss: 1.6695 - val_accuracy: 0.6920\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6272 - accuracy: 0.6972 - val_loss: 1.6542 - val_accuracy: 0.6910\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6184 - accuracy: 0.6984 - val_loss: 1.6529 - val_accuracy: 0.6920\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6097 - accuracy: 0.7008 - val_loss: 1.6365 - val_accuracy: 0.6910\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.6017 - accuracy: 0.7001 - val_loss: 1.6300 - val_accuracy: 0.6920\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5930 - accuracy: 0.7005 - val_loss: 1.6241 - val_accuracy: 0.6860\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.5847 - accuracy: 0.7025 - val_loss: 1.6154 - val_accuracy: 0.6900\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.5767 - accuracy: 0.7019 - val_loss: 1.6075 - val_accuracy: 0.6950\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5690 - accuracy: 0.7016 - val_loss: 1.5954 - val_accuracy: 0.6970\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5611 - accuracy: 0.7016 - val_loss: 1.5920 - val_accuracy: 0.6910\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5539 - accuracy: 0.7041 - val_loss: 1.5829 - val_accuracy: 0.6930\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5454 - accuracy: 0.7036 - val_loss: 1.5779 - val_accuracy: 0.6930\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5384 - accuracy: 0.7032 - val_loss: 1.5673 - val_accuracy: 0.6960\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5310 - accuracy: 0.7049 - val_loss: 1.5645 - val_accuracy: 0.6950\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5234 - accuracy: 0.7051 - val_loss: 1.5634 - val_accuracy: 0.6940\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5161 - accuracy: 0.7060 - val_loss: 1.5478 - val_accuracy: 0.6980\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.5095 - accuracy: 0.7063 - val_loss: 1.5447 - val_accuracy: 0.6940\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.5022 - accuracy: 0.7060 - val_loss: 1.5314 - val_accuracy: 0.7000\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4952 - accuracy: 0.7072 - val_loss: 1.5268 - val_accuracy: 0.6950\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.4883 - accuracy: 0.7084 - val_loss: 1.5224 - val_accuracy: 0.6940\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4816 - accuracy: 0.7079 - val_loss: 1.5172 - val_accuracy: 0.6940\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4752 - accuracy: 0.7079 - val_loss: 1.5064 - val_accuracy: 0.7020\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4687 - accuracy: 0.7080 - val_loss: 1.5005 - val_accuracy: 0.6980\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4622 - accuracy: 0.7088 - val_loss: 1.4942 - val_accuracy: 0.7000\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4553 - accuracy: 0.7093 - val_loss: 1.4874 - val_accuracy: 0.7010\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.4498 - accuracy: 0.7081 - val_loss: 1.4845 - val_accuracy: 0.7020\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4431 - accuracy: 0.7105 - val_loss: 1.4819 - val_accuracy: 0.7000\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4370 - accuracy: 0.7115 - val_loss: 1.4721 - val_accuracy: 0.7020\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4312 - accuracy: 0.7095 - val_loss: 1.4697 - val_accuracy: 0.7020\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4251 - accuracy: 0.7108 - val_loss: 1.4606 - val_accuracy: 0.7030\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4194 - accuracy: 0.7099 - val_loss: 1.4553 - val_accuracy: 0.7030\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4127 - accuracy: 0.7141 - val_loss: 1.4494 - val_accuracy: 0.7010\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4076 - accuracy: 0.7093 - val_loss: 1.4459 - val_accuracy: 0.7060\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4020 - accuracy: 0.7120 - val_loss: 1.4385 - val_accuracy: 0.7020\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3962 - accuracy: 0.7119 - val_loss: 1.4329 - val_accuracy: 0.7090\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3910 - accuracy: 0.7120 - val_loss: 1.4293 - val_accuracy: 0.7040\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3856 - accuracy: 0.7136 - val_loss: 1.4251 - val_accuracy: 0.7080\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3800 - accuracy: 0.7127 - val_loss: 1.4170 - val_accuracy: 0.7050\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3750 - accuracy: 0.7120 - val_loss: 1.4106 - val_accuracy: 0.7050\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3691 - accuracy: 0.7128 - val_loss: 1.4083 - val_accuracy: 0.7100\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3642 - accuracy: 0.7143 - val_loss: 1.4079 - val_accuracy: 0.7070\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3590 - accuracy: 0.7149 - val_loss: 1.3967 - val_accuracy: 0.7050\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3537 - accuracy: 0.7151 - val_loss: 1.3927 - val_accuracy: 0.7070\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3490 - accuracy: 0.7124 - val_loss: 1.3876 - val_accuracy: 0.7070\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3437 - accuracy: 0.7139 - val_loss: 1.3812 - val_accuracy: 0.7090\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3386 - accuracy: 0.7171 - val_loss: 1.3790 - val_accuracy: 0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/120\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3344 - accuracy: 0.7151 - val_loss: 1.3790 - val_accuracy: 0.7070\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3290 - accuracy: 0.7165 - val_loss: 1.3764 - val_accuracy: 0.7130\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3243 - accuracy: 0.7188 - val_loss: 1.3620 - val_accuracy: 0.7070\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3196 - accuracy: 0.7155 - val_loss: 1.3592 - val_accuracy: 0.7120\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3147 - accuracy: 0.7192 - val_loss: 1.3548 - val_accuracy: 0.7080\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to get and visualize the model's `.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-d9f7a74ada55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0macc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL1_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL1_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer.\n",
    "\n",
    "To complete our comparison, let's use `model.evaluate()` again on the appropriate variables to compare results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 0s 447us/step - loss: 1.3141 - accuracy: 0.7172\n",
      "47/47 [==============================] - 0s 447us/step - loss: 1.3513 - accuracy: 0.6987\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.314137578010559, 0.717199981212616]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output: [1.3186310468037923, 0.72266666663487755]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3513418436050415, 0.6986666917800903]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [1.3541648308436076, 0.70800000031789145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout Regularization\n",
    "\n",
    "Dropout Regularization is accomplished by adding in an additional `Dropout` layer wherever we want to use it, and providing a percentage value for how likely any given neuron is to get \"dropped out\" during this layer. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `Dropout` from `keras.layers`\n",
    "* Recreate the same network we have above, but this time without any L1 or L2 regularization\n",
    "* Add a `Dropout` layer between hidden layer 1 and hidden layer 2.  This should have a dropout chance of `0.3`.\n",
    "* Add a `Dropout` layer between hidden layer 2 and the output layer.  This should have a dropout chance of `0.3`.\n",
    "* Compile the model with the exact same hyperparameters as all other models we've built. \n",
    "* Fit the model with the same hyperparameters we've used above.  But this time, train the model for `200` epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 1.9691 - accuracy: 0.1444 - val_loss: 1.9384 - val_accuracy: 0.1660\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.9405 - accuracy: 0.1700 - val_loss: 1.9225 - val_accuracy: 0.1810\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.9259 - accuracy: 0.1843 - val_loss: 1.9097 - val_accuracy: 0.2130\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.9095 - accuracy: 0.2035 - val_loss: 1.8975 - val_accuracy: 0.2220\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8947 - accuracy: 0.2256 - val_loss: 1.8839 - val_accuracy: 0.2350\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8837 - accuracy: 0.2253 - val_loss: 1.8689 - val_accuracy: 0.2530\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8719 - accuracy: 0.2367 - val_loss: 1.8522 - val_accuracy: 0.2800\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8544 - accuracy: 0.2525 - val_loss: 1.8332 - val_accuracy: 0.2770\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8333 - accuracy: 0.2711 - val_loss: 1.8101 - val_accuracy: 0.2910\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.8148 - accuracy: 0.2780 - val_loss: 1.7845 - val_accuracy: 0.3130\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.7926 - accuracy: 0.2917 - val_loss: 1.7582 - val_accuracy: 0.3270\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.7714 - accuracy: 0.3029 - val_loss: 1.7290 - val_accuracy: 0.3490\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.7494 - accuracy: 0.3169 - val_loss: 1.6999 - val_accuracy: 0.3680\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.7276 - accuracy: 0.3259 - val_loss: 1.6710 - val_accuracy: 0.3900\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.7109 - accuracy: 0.3345 - val_loss: 1.6432 - val_accuracy: 0.4130\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6834 - accuracy: 0.3432 - val_loss: 1.6133 - val_accuracy: 0.4340\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6578 - accuracy: 0.3517 - val_loss: 1.5853 - val_accuracy: 0.4520\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6404 - accuracy: 0.3625 - val_loss: 1.5587 - val_accuracy: 0.4630\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6114 - accuracy: 0.3804 - val_loss: 1.5311 - val_accuracy: 0.4750\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.5874 - accuracy: 0.3896 - val_loss: 1.5021 - val_accuracy: 0.4900\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.5664 - accuracy: 0.3971 - val_loss: 1.4766 - val_accuracy: 0.5030\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.5449 - accuracy: 0.4027 - val_loss: 1.4495 - val_accuracy: 0.5140\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.5203 - accuracy: 0.4169 - val_loss: 1.4222 - val_accuracy: 0.5250\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.4962 - accuracy: 0.4260 - val_loss: 1.3967 - val_accuracy: 0.5390\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.4766 - accuracy: 0.4443 - val_loss: 1.3726 - val_accuracy: 0.5710\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.4573 - accuracy: 0.4451 - val_loss: 1.3501 - val_accuracy: 0.5700\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4387 - accuracy: 0.4568 - val_loss: 1.3263 - val_accuracy: 0.5750\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4208 - accuracy: 0.4683 - val_loss: 1.3041 - val_accuracy: 0.5910\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4049 - accuracy: 0.4697 - val_loss: 1.2808 - val_accuracy: 0.6070\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3757 - accuracy: 0.4871 - val_loss: 1.2578 - val_accuracy: 0.6190\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3700 - accuracy: 0.4853 - val_loss: 1.2390 - val_accuracy: 0.6260\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3598 - accuracy: 0.4865 - val_loss: 1.2208 - val_accuracy: 0.6420\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3224 - accuracy: 0.5068 - val_loss: 1.1987 - val_accuracy: 0.6510\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3039 - accuracy: 0.5157 - val_loss: 1.1816 - val_accuracy: 0.6590\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3026 - accuracy: 0.5127 - val_loss: 1.1636 - val_accuracy: 0.6620\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.2798 - accuracy: 0.5156 - val_loss: 1.1471 - val_accuracy: 0.6630\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.2496 - accuracy: 0.5387 - val_loss: 1.1257 - val_accuracy: 0.6700\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.2450 - accuracy: 0.5331 - val_loss: 1.1103 - val_accuracy: 0.6720\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.2224 - accuracy: 0.5483 - val_loss: 1.0929 - val_accuracy: 0.6780\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.2114 - accuracy: 0.5513 - val_loss: 1.0776 - val_accuracy: 0.6800\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1959 - accuracy: 0.5599 - val_loss: 1.0631 - val_accuracy: 0.6840\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1810 - accuracy: 0.5628 - val_loss: 1.0458 - val_accuracy: 0.6820\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1647 - accuracy: 0.5784 - val_loss: 1.0334 - val_accuracy: 0.6890\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1545 - accuracy: 0.5732 - val_loss: 1.0170 - val_accuracy: 0.6870\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.1367 - accuracy: 0.5799 - val_loss: 1.0039 - val_accuracy: 0.6930\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1171 - accuracy: 0.5839 - val_loss: 0.9897 - val_accuracy: 0.6930\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.1195 - accuracy: 0.5807 - val_loss: 0.9777 - val_accuracy: 0.6960\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0966 - accuracy: 0.6044 - val_loss: 0.9627 - val_accuracy: 0.7000\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.0791 - accuracy: 0.6023 - val_loss: 0.9555 - val_accuracy: 0.6960\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0831 - accuracy: 0.6037 - val_loss: 0.9423 - val_accuracy: 0.7030\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0643 - accuracy: 0.6113 - val_loss: 0.9310 - val_accuracy: 0.7050\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0568 - accuracy: 0.6104 - val_loss: 0.9203 - val_accuracy: 0.7050\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0364 - accuracy: 0.6200 - val_loss: 0.9100 - val_accuracy: 0.7070\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0339 - accuracy: 0.6180 - val_loss: 0.8989 - val_accuracy: 0.7100\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0307 - accuracy: 0.6193 - val_loss: 0.8920 - val_accuracy: 0.7090\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0254 - accuracy: 0.6100 - val_loss: 0.8854 - val_accuracy: 0.7090\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.0037 - accuracy: 0.6205 - val_loss: 0.8762 - val_accuracy: 0.7100\n",
      "Epoch 58/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9880 - accuracy: 0.6268 - val_loss: 0.8666 - val_accuracy: 0.7140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9708 - accuracy: 0.6363 - val_loss: 0.8580 - val_accuracy: 0.7120\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9751 - accuracy: 0.6357 - val_loss: 0.8490 - val_accuracy: 0.7140\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9734 - accuracy: 0.6371 - val_loss: 0.8470 - val_accuracy: 0.7130\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9624 - accuracy: 0.6457 - val_loss: 0.8391 - val_accuracy: 0.7150\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9439 - accuracy: 0.6505 - val_loss: 0.8297 - val_accuracy: 0.7140\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9385 - accuracy: 0.6483 - val_loss: 0.8219 - val_accuracy: 0.7150\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9426 - accuracy: 0.6464 - val_loss: 0.8169 - val_accuracy: 0.7150\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.9243 - accuracy: 0.6517 - val_loss: 0.8108 - val_accuracy: 0.7160\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9333 - accuracy: 0.6512 - val_loss: 0.8056 - val_accuracy: 0.7170\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9214 - accuracy: 0.6564 - val_loss: 0.8014 - val_accuracy: 0.7160\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.9188 - accuracy: 0.6573 - val_loss: 0.7975 - val_accuracy: 0.7210\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8978 - accuracy: 0.6671 - val_loss: 0.7905 - val_accuracy: 0.7210\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8864 - accuracy: 0.6725 - val_loss: 0.7838 - val_accuracy: 0.7230\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8884 - accuracy: 0.6696 - val_loss: 0.7800 - val_accuracy: 0.7220\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8905 - accuracy: 0.6727 - val_loss: 0.7766 - val_accuracy: 0.7240\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8639 - accuracy: 0.6772 - val_loss: 0.7701 - val_accuracy: 0.7230\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8661 - accuracy: 0.6775 - val_loss: 0.7643 - val_accuracy: 0.7260\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8659 - accuracy: 0.6803 - val_loss: 0.7632 - val_accuracy: 0.7240\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8636 - accuracy: 0.6801 - val_loss: 0.7595 - val_accuracy: 0.7230\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8507 - accuracy: 0.6879 - val_loss: 0.7551 - val_accuracy: 0.7270\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8479 - accuracy: 0.6832 - val_loss: 0.7535 - val_accuracy: 0.7270\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8409 - accuracy: 0.6903 - val_loss: 0.7509 - val_accuracy: 0.7330\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8450 - accuracy: 0.6887 - val_loss: 0.7448 - val_accuracy: 0.7300\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8263 - accuracy: 0.6905 - val_loss: 0.7436 - val_accuracy: 0.7320\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8227 - accuracy: 0.6960 - val_loss: 0.7379 - val_accuracy: 0.7320\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8149 - accuracy: 0.6968 - val_loss: 0.7347 - val_accuracy: 0.7280\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7984 - accuracy: 0.7021 - val_loss: 0.7318 - val_accuracy: 0.7370\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.8020 - accuracy: 0.7029 - val_loss: 0.7307 - val_accuracy: 0.7380\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.8159 - accuracy: 0.7003 - val_loss: 0.7287 - val_accuracy: 0.7380\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7973 - accuracy: 0.7027 - val_loss: 0.7226 - val_accuracy: 0.7370\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7888 - accuracy: 0.7057 - val_loss: 0.7214 - val_accuracy: 0.7380\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.7838 - accuracy: 0.71 - 0s 4ms/step - loss: 0.7849 - accuracy: 0.7093 - val_loss: 0.7179 - val_accuracy: 0.7360\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7905 - accuracy: 0.7057 - val_loss: 0.7161 - val_accuracy: 0.7360\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7700 - accuracy: 0.7101 - val_loss: 0.7156 - val_accuracy: 0.7390\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7727 - accuracy: 0.7116 - val_loss: 0.7114 - val_accuracy: 0.7380\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7636 - accuracy: 0.7141 - val_loss: 0.7076 - val_accuracy: 0.7390\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7784 - accuracy: 0.7119 - val_loss: 0.7063 - val_accuracy: 0.7370\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7611 - accuracy: 0.7165 - val_loss: 0.7035 - val_accuracy: 0.7390\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7566 - accuracy: 0.7165 - val_loss: 0.7052 - val_accuracy: 0.7370\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7498 - accuracy: 0.7231 - val_loss: 0.7002 - val_accuracy: 0.7420\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7496 - accuracy: 0.7177 - val_loss: 0.6995 - val_accuracy: 0.7450\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7464 - accuracy: 0.7193 - val_loss: 0.6961 - val_accuracy: 0.7380\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7483 - accuracy: 0.7148 - val_loss: 0.6958 - val_accuracy: 0.7450\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7476 - accuracy: 0.7172 - val_loss: 0.6965 - val_accuracy: 0.7440\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7244 - accuracy: 0.7264 - val_loss: 0.6934 - val_accuracy: 0.7410\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7289 - accuracy: 0.7255 - val_loss: 0.6896 - val_accuracy: 0.7480\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7214 - accuracy: 0.7321 - val_loss: 0.6874 - val_accuracy: 0.7470\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7208 - accuracy: 0.7265 - val_loss: 0.6860 - val_accuracy: 0.7490\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7209 - accuracy: 0.7356 - val_loss: 0.6834 - val_accuracy: 0.7480\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7184 - accuracy: 0.7299 - val_loss: 0.6874 - val_accuracy: 0.7500\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7138 - accuracy: 0.7336 - val_loss: 0.6823 - val_accuracy: 0.7480\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7188 - accuracy: 0.7308 - val_loss: 0.6809 - val_accuracy: 0.7490\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.7034 - accuracy: 0.7376 - val_loss: 0.6801 - val_accuracy: 0.7520\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.7030 - accuracy: 0.7397 - val_loss: 0.6776 - val_accuracy: 0.7470\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.7448 - val_loss: 0.6760 - val_accuracy: 0.7520\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.7427 - val_loss: 0.6756 - val_accuracy: 0.7510\n",
      "Epoch 115/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.7447 - val_loss: 0.6737 - val_accuracy: 0.7510\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6964 - accuracy: 0.7392 - val_loss: 0.6754 - val_accuracy: 0.7500\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.7447 - val_loss: 0.6721 - val_accuracy: 0.7500\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.7463 - val_loss: 0.6706 - val_accuracy: 0.7500\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.7436 - val_loss: 0.6703 - val_accuracy: 0.7510\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6775 - accuracy: 0.7499 - val_loss: 0.6691 - val_accuracy: 0.7500\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6820 - accuracy: 0.7427 - val_loss: 0.6691 - val_accuracy: 0.7550\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6690 - accuracy: 0.7521 - val_loss: 0.6678 - val_accuracy: 0.7530\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6668 - accuracy: 0.7559 - val_loss: 0.6667 - val_accuracy: 0.7520\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6697 - accuracy: 0.7504 - val_loss: 0.6696 - val_accuracy: 0.7500\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.7520 - val_loss: 0.6647 - val_accuracy: 0.7520\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6578 - accuracy: 0.7557 - val_loss: 0.6623 - val_accuracy: 0.7470\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.7539 - val_loss: 0.6630 - val_accuracy: 0.7510\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6557 - accuracy: 0.7525 - val_loss: 0.6620 - val_accuracy: 0.7510\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6529 - accuracy: 0.7551 - val_loss: 0.6607 - val_accuracy: 0.7560\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6503 - accuracy: 0.7541 - val_loss: 0.6601 - val_accuracy: 0.7550\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6413 - accuracy: 0.7599 - val_loss: 0.6599 - val_accuracy: 0.7570\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 0.7673 - val_loss: 0.6596 - val_accuracy: 0.7530\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6470 - accuracy: 0.7597 - val_loss: 0.6588 - val_accuracy: 0.7540\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6402 - accuracy: 0.7600 - val_loss: 0.6568 - val_accuracy: 0.7570\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6312 - accuracy: 0.7624 - val_loss: 0.6582 - val_accuracy: 0.7550\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6324 - accuracy: 0.7609 - val_loss: 0.6590 - val_accuracy: 0.7540\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6317 - accuracy: 0.7663 - val_loss: 0.6575 - val_accuracy: 0.7560\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6272 - accuracy: 0.7667 - val_loss: 0.6554 - val_accuracy: 0.7550\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.6258 - accuracy: 0.7663 - val_loss: 0.6554 - val_accuracy: 0.7580\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6271 - accuracy: 0.7712 - val_loss: 0.6547 - val_accuracy: 0.7580\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6318 - accuracy: 0.7617 - val_loss: 0.6553 - val_accuracy: 0.7550\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6179 - accuracy: 0.7701 - val_loss: 0.6551 - val_accuracy: 0.7560\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6166 - accuracy: 0.7692 - val_loss: 0.6538 - val_accuracy: 0.7560\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6169 - accuracy: 0.7701 - val_loss: 0.6524 - val_accuracy: 0.7540\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6090 - accuracy: 0.7729 - val_loss: 0.6527 - val_accuracy: 0.7580\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6180 - accuracy: 0.7715 - val_loss: 0.6505 - val_accuracy: 0.7590\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6179 - accuracy: 0.7701 - val_loss: 0.6499 - val_accuracy: 0.7540\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6092 - accuracy: 0.7653 - val_loss: 0.6496 - val_accuracy: 0.7570\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6045 - accuracy: 0.7671 - val_loss: 0.6489 - val_accuracy: 0.7550\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.6100 - accuracy: 0.7703 - val_loss: 0.6500 - val_accuracy: 0.7570\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5906 - accuracy: 0.7867 - val_loss: 0.6497 - val_accuracy: 0.7550\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5979 - accuracy: 0.7769 - val_loss: 0.6486 - val_accuracy: 0.7590\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.7816 - val_loss: 0.6482 - val_accuracy: 0.7550\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5939 - accuracy: 0.7765 - val_loss: 0.6475 - val_accuracy: 0.7610\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5881 - accuracy: 0.7796 - val_loss: 0.6475 - val_accuracy: 0.7560\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5828 - accuracy: 0.7799 - val_loss: 0.6476 - val_accuracy: 0.7580\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5961 - accuracy: 0.7812 - val_loss: 0.6481 - val_accuracy: 0.7600\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5948 - accuracy: 0.7776 - val_loss: 0.6471 - val_accuracy: 0.7580\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5829 - accuracy: 0.7753 - val_loss: 0.6472 - val_accuracy: 0.7620\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5861 - accuracy: 0.7827 - val_loss: 0.6475 - val_accuracy: 0.7560\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5758 - accuracy: 0.7848 - val_loss: 0.6480 - val_accuracy: 0.7610\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5760 - accuracy: 0.7864 - val_loss: 0.6446 - val_accuracy: 0.7580\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5821 - accuracy: 0.7821 - val_loss: 0.6442 - val_accuracy: 0.7590\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5642 - accuracy: 0.7839 - val_loss: 0.6465 - val_accuracy: 0.7570\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5670 - accuracy: 0.7877 - val_loss: 0.6456 - val_accuracy: 0.7620\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5704 - accuracy: 0.7844 - val_loss: 0.6446 - val_accuracy: 0.7590\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5719 - accuracy: 0.7857 - val_loss: 0.6448 - val_accuracy: 0.7600\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5537 - accuracy: 0.7861 - val_loss: 0.6440 - val_accuracy: 0.7600\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5655 - accuracy: 0.7820 - val_loss: 0.6444 - val_accuracy: 0.7610\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5645 - accuracy: 0.7857 - val_loss: 0.6433 - val_accuracy: 0.7590\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5630 - accuracy: 0.7939 - val_loss: 0.6438 - val_accuracy: 0.7610\n",
      "Epoch 172/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5490 - accuracy: 0.7903 - val_loss: 0.6434 - val_accuracy: 0.7590\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5583 - accuracy: 0.7921 - val_loss: 0.6428 - val_accuracy: 0.7580\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5584 - accuracy: 0.7901 - val_loss: 0.6436 - val_accuracy: 0.7590\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5522 - accuracy: 0.7885 - val_loss: 0.6431 - val_accuracy: 0.7630\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5508 - accuracy: 0.7959 - val_loss: 0.6448 - val_accuracy: 0.7620\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5478 - accuracy: 0.7917 - val_loss: 0.6428 - val_accuracy: 0.7600\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5487 - accuracy: 0.7937 - val_loss: 0.6428 - val_accuracy: 0.7600\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5403 - accuracy: 0.7999 - val_loss: 0.6440 - val_accuracy: 0.7610\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5375 - accuracy: 0.8000 - val_loss: 0.6414 - val_accuracy: 0.7630\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5464 - accuracy: 0.7936 - val_loss: 0.6426 - val_accuracy: 0.7600\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5316 - accuracy: 0.8019 - val_loss: 0.6433 - val_accuracy: 0.7630\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5389 - accuracy: 0.7988 - val_loss: 0.6426 - val_accuracy: 0.7610\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5322 - accuracy: 0.8029 - val_loss: 0.6420 - val_accuracy: 0.7650\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5233 - accuracy: 0.8113 - val_loss: 0.6435 - val_accuracy: 0.7640\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5262 - accuracy: 0.8088 - val_loss: 0.6432 - val_accuracy: 0.7620\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5335 - accuracy: 0.7993 - val_loss: 0.6433 - val_accuracy: 0.7620\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5193 - accuracy: 0.8053 - val_loss: 0.6422 - val_accuracy: 0.7630\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5193 - accuracy: 0.8073 - val_loss: 0.6412 - val_accuracy: 0.7630\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5240 - accuracy: 0.8020 - val_loss: 0.6417 - val_accuracy: 0.7660\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5130 - accuracy: 0.8013 - val_loss: 0.6434 - val_accuracy: 0.7590\n",
      "Epoch 192/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5214 - accuracy: 0.8053 - val_loss: 0.6457 - val_accuracy: 0.7590\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5136 - accuracy: 0.8084 - val_loss: 0.6428 - val_accuracy: 0.7630\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5136 - accuracy: 0.8093 - val_loss: 0.6404 - val_accuracy: 0.7630\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5168 - accuracy: 0.8069 - val_loss: 0.6409 - val_accuracy: 0.7600\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5179 - accuracy: 0.8032 - val_loss: 0.6405 - val_accuracy: 0.7660\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5146 - accuracy: 0.8040 - val_loss: 0.6420 - val_accuracy: 0.7630\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5132 - accuracy: 0.8064 - val_loss: 0.6409 - val_accuracy: 0.7650\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.5033 - accuracy: 0.8100 - val_loss: 0.6419 - val_accuracy: 0.7680\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.8120 - val_loss: 0.6419 - val_accuracy: 0.7670\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the results from `model.evaluate` to see how this change has affected our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/235 [..............................] - ETA: 0s - loss: 0.3117 - accuracy: 0.8750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "235/235 [==============================] - 0s 506us/step - loss: 0.3448 - accuracy: 0.8896\n",
      "47/47 [==============================] - 0s 511us/step - loss: 0.6499 - accuracy: 0.7633\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34477952122688293, 0.8895999789237976]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Results: [0.36925017188787462, 0.88026666666666664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6498584151268005, 0.7633333206176758]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Results: [0.69210424280166627, 0.74333333365122478]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! However, the variance did become higher again, compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  More Training Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to high variance is to just get more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets.\n",
    "\n",
    "Run the cell below to preprocess our entire dataset, instead of just working with a subset of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the first model that we built, without any regularization or dropout layers included. \n",
    "\n",
    "Train this model for 120 epochs.  All other hyperparameters should stay the same.  Store the fitted model inside of `moredata_model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 1.9111 - accuracy: 0.2181 - val_loss: 1.8662 - val_accuracy: 0.2740\n",
      "Epoch 2/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 1.8052 - accuracy: 0.3260 - val_loss: 1.7266 - val_accuracy: 0.3807\n",
      "Epoch 3/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 1.6286 - accuracy: 0.4352 - val_loss: 1.5109 - val_accuracy: 0.4977\n",
      "Epoch 4/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 1.3999 - accuracy: 0.5411 - val_loss: 1.2807 - val_accuracy: 0.5950\n",
      "Epoch 5/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 1.1870 - accuracy: 0.6292 - val_loss: 1.0927 - val_accuracy: 0.6673\n",
      "Epoch 6/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 1.0228 - accuracy: 0.6786 - val_loss: 0.9575 - val_accuracy: 0.7060\n",
      "Epoch 7/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.9066 - accuracy: 0.7034 - val_loss: 0.8629 - val_accuracy: 0.7190\n",
      "Epoch 8/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.8259 - accuracy: 0.7210 - val_loss: 0.7987 - val_accuracy: 0.7280\n",
      "Epoch 9/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.7692 - accuracy: 0.7338 - val_loss: 0.7526 - val_accuracy: 0.7373\n",
      "Epoch 10/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.7278 - accuracy: 0.7426 - val_loss: 0.7176 - val_accuracy: 0.7470\n",
      "Epoch 11/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.6964 - accuracy: 0.7495 - val_loss: 0.6916 - val_accuracy: 0.7527\n",
      "Epoch 12/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.6716 - accuracy: 0.7576 - val_loss: 0.6713 - val_accuracy: 0.7570\n",
      "Epoch 13/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.6517 - accuracy: 0.7623 - val_loss: 0.6558 - val_accuracy: 0.7610\n",
      "Epoch 14/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.6344 - accuracy: 0.7677 - val_loss: 0.6446 - val_accuracy: 0.7633\n",
      "Epoch 15/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.6199 - accuracy: 0.7733 - val_loss: 0.6326 - val_accuracy: 0.7717\n",
      "Epoch 16/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.6072 - accuracy: 0.7769 - val_loss: 0.6223 - val_accuracy: 0.7713\n",
      "Epoch 17/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5958 - accuracy: 0.7813 - val_loss: 0.6139 - val_accuracy: 0.7737\n",
      "Epoch 18/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5857 - accuracy: 0.7857 - val_loss: 0.6062 - val_accuracy: 0.7783\n",
      "Epoch 19/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5764 - accuracy: 0.7878 - val_loss: 0.5994 - val_accuracy: 0.7800\n",
      "Epoch 20/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5675 - accuracy: 0.7908 - val_loss: 0.5959 - val_accuracy: 0.7810\n",
      "Epoch 21/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5595 - accuracy: 0.7944 - val_loss: 0.5889 - val_accuracy: 0.7843\n",
      "Epoch 22/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5521 - accuracy: 0.7972 - val_loss: 0.5884 - val_accuracy: 0.7833\n",
      "Epoch 23/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5448 - accuracy: 0.8014 - val_loss: 0.5838 - val_accuracy: 0.7833\n",
      "Epoch 24/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5381 - accuracy: 0.8023 - val_loss: 0.5762 - val_accuracy: 0.7890\n",
      "Epoch 25/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5319 - accuracy: 0.8045 - val_loss: 0.5715 - val_accuracy: 0.7883\n",
      "Epoch 26/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5260 - accuracy: 0.8083 - val_loss: 0.5692 - val_accuracy: 0.7913\n",
      "Epoch 27/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5200 - accuracy: 0.8106 - val_loss: 0.5647 - val_accuracy: 0.7903\n",
      "Epoch 28/120\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.5144 - accuracy: 0.8130 - val_loss: 0.5633 - val_accuracy: 0.7927\n",
      "Epoch 29/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5091 - accuracy: 0.8146 - val_loss: 0.5592 - val_accuracy: 0.7953\n",
      "Epoch 30/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.5038 - accuracy: 0.8170 - val_loss: 0.5568 - val_accuracy: 0.7973\n",
      "Epoch 31/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4992 - accuracy: 0.8193 - val_loss: 0.5556 - val_accuracy: 0.7977\n",
      "Epoch 32/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4943 - accuracy: 0.8204 - val_loss: 0.5527 - val_accuracy: 0.7953\n",
      "Epoch 33/120\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.4893 - accuracy: 0.8237 - val_loss: 0.5508 - val_accuracy: 0.7970\n",
      "Epoch 34/120\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.4850 - accuracy: 0.8258 - val_loss: 0.5525 - val_accuracy: 0.7987\n",
      "Epoch 35/120\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.4809 - accuracy: 0.8265 - val_loss: 0.5477 - val_accuracy: 0.8017\n",
      "Epoch 36/120\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.4764 - accuracy: 0.8283 - val_loss: 0.5460 - val_accuracy: 0.8030\n",
      "Epoch 37/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4726 - accuracy: 0.8305 - val_loss: 0.5450 - val_accuracy: 0.8023\n",
      "Epoch 38/120\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.4687 - accuracy: 0.8315 - val_loss: 0.5426 - val_accuracy: 0.8020\n",
      "Epoch 39/120\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.4645 - accuracy: 0.8335 - val_loss: 0.5430 - val_accuracy: 0.8040\n",
      "Epoch 40/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.8345 - val_loss: 0.5402 - val_accuracy: 0.8027\n",
      "Epoch 41/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4573 - accuracy: 0.8365 - val_loss: 0.5410 - val_accuracy: 0.8060\n",
      "Epoch 42/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4540 - accuracy: 0.8373 - val_loss: 0.5380 - val_accuracy: 0.8060\n",
      "Epoch 43/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4505 - accuracy: 0.8394 - val_loss: 0.5401 - val_accuracy: 0.8053\n",
      "Epoch 44/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4470 - accuracy: 0.8394 - val_loss: 0.5373 - val_accuracy: 0.8087\n",
      "Epoch 45/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4442 - accuracy: 0.8414 - val_loss: 0.5362 - val_accuracy: 0.8063\n",
      "Epoch 46/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4405 - accuracy: 0.8429 - val_loss: 0.5358 - val_accuracy: 0.8080\n",
      "Epoch 47/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8435 - val_loss: 0.5352 - val_accuracy: 0.8083\n",
      "Epoch 48/120\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.4347 - accuracy: 0.8456 - val_loss: 0.5346 - val_accuracy: 0.8107\n",
      "Epoch 49/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.8467 - val_loss: 0.5340 - val_accuracy: 0.8110\n",
      "Epoch 50/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4289 - accuracy: 0.8473 - val_loss: 0.5333 - val_accuracy: 0.8110\n",
      "Epoch 51/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4260 - accuracy: 0.8484 - val_loss: 0.5335 - val_accuracy: 0.8117\n",
      "Epoch 52/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8506 - val_loss: 0.5336 - val_accuracy: 0.8093\n",
      "Epoch 53/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4206 - accuracy: 0.8499 - val_loss: 0.5320 - val_accuracy: 0.8120\n",
      "Epoch 54/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4178 - accuracy: 0.8513 - val_loss: 0.5332 - val_accuracy: 0.8110\n",
      "Epoch 55/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4155 - accuracy: 0.8531 - val_loss: 0.5337 - val_accuracy: 0.8093\n",
      "Epoch 56/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4129 - accuracy: 0.8544 - val_loss: 0.5315 - val_accuracy: 0.8107\n",
      "Epoch 57/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4104 - accuracy: 0.8558 - val_loss: 0.5340 - val_accuracy: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4078 - accuracy: 0.8552 - val_loss: 0.5378 - val_accuracy: 0.8053\n",
      "Epoch 59/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8572 - val_loss: 0.5324 - val_accuracy: 0.8103\n",
      "Epoch 60/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4032 - accuracy: 0.8584 - val_loss: 0.5346 - val_accuracy: 0.8087\n",
      "Epoch 61/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8588 - val_loss: 0.5328 - val_accuracy: 0.8107\n",
      "Epoch 62/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8600 - val_loss: 0.5323 - val_accuracy: 0.8110\n",
      "Epoch 63/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3963 - accuracy: 0.8602 - val_loss: 0.5351 - val_accuracy: 0.8093\n",
      "Epoch 64/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3942 - accuracy: 0.8612 - val_loss: 0.5329 - val_accuracy: 0.8117\n",
      "Epoch 65/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8627 - val_loss: 0.5367 - val_accuracy: 0.8067\n",
      "Epoch 66/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8636 - val_loss: 0.5327 - val_accuracy: 0.8093\n",
      "Epoch 67/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3880 - accuracy: 0.8637 - val_loss: 0.5329 - val_accuracy: 0.8080\n",
      "Epoch 68/120\n",
      "129/129 [==============================] - ETA: 0s - loss: 0.3834 - accuracy: 0.86 - 0s 3ms/step - loss: 0.3859 - accuracy: 0.8639 - val_loss: 0.5331 - val_accuracy: 0.8090\n",
      "Epoch 69/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3839 - accuracy: 0.8654 - val_loss: 0.5378 - val_accuracy: 0.8080\n",
      "Epoch 70/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.8656 - val_loss: 0.5350 - val_accuracy: 0.8080\n",
      "Epoch 71/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8671 - val_loss: 0.5340 - val_accuracy: 0.8083\n",
      "Epoch 72/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8669 - val_loss: 0.5361 - val_accuracy: 0.8083\n",
      "Epoch 73/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3765 - accuracy: 0.8683 - val_loss: 0.5357 - val_accuracy: 0.8087\n",
      "Epoch 74/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8688 - val_loss: 0.5366 - val_accuracy: 0.8093\n",
      "Epoch 75/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3726 - accuracy: 0.8698 - val_loss: 0.5384 - val_accuracy: 0.8087\n",
      "Epoch 76/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.8699 - val_loss: 0.5384 - val_accuracy: 0.8097\n",
      "Epoch 77/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8708 - val_loss: 0.5367 - val_accuracy: 0.8083\n",
      "Epoch 78/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8720 - val_loss: 0.5367 - val_accuracy: 0.8100\n",
      "Epoch 79/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3662 - accuracy: 0.8717 - val_loss: 0.5388 - val_accuracy: 0.8100\n",
      "Epoch 80/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3642 - accuracy: 0.8732 - val_loss: 0.5392 - val_accuracy: 0.8093\n",
      "Epoch 81/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3625 - accuracy: 0.8741 - val_loss: 0.5396 - val_accuracy: 0.8077\n",
      "Epoch 82/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8735 - val_loss: 0.5419 - val_accuracy: 0.8070\n",
      "Epoch 83/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8749 - val_loss: 0.5434 - val_accuracy: 0.8080\n",
      "Epoch 84/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3577 - accuracy: 0.8750 - val_loss: 0.5421 - val_accuracy: 0.8083\n",
      "Epoch 85/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3562 - accuracy: 0.8762 - val_loss: 0.5435 - val_accuracy: 0.8093\n",
      "Epoch 86/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.8762 - val_loss: 0.5416 - val_accuracy: 0.8067\n",
      "Epoch 87/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3528 - accuracy: 0.8774 - val_loss: 0.5454 - val_accuracy: 0.8077\n",
      "Epoch 88/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3516 - accuracy: 0.8768 - val_loss: 0.5439 - val_accuracy: 0.8070\n",
      "Epoch 89/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3501 - accuracy: 0.8776 - val_loss: 0.5461 - val_accuracy: 0.8080\n",
      "Epoch 90/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3485 - accuracy: 0.8791 - val_loss: 0.5450 - val_accuracy: 0.8070\n",
      "Epoch 91/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.8787 - val_loss: 0.5472 - val_accuracy: 0.8093\n",
      "Epoch 92/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3453 - accuracy: 0.8803 - val_loss: 0.5491 - val_accuracy: 0.8077\n",
      "Epoch 93/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3438 - accuracy: 0.8795 - val_loss: 0.5470 - val_accuracy: 0.8093\n",
      "Epoch 94/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3427 - accuracy: 0.8815 - val_loss: 0.5496 - val_accuracy: 0.8117\n",
      "Epoch 95/120\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.3414 - accuracy: 0.8814 - val_loss: 0.5491 - val_accuracy: 0.8080\n",
      "Epoch 96/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3399 - accuracy: 0.8817 - val_loss: 0.5497 - val_accuracy: 0.8067\n",
      "Epoch 97/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8827 - val_loss: 0.5501 - val_accuracy: 0.8083\n",
      "Epoch 98/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8834 - val_loss: 0.5517 - val_accuracy: 0.8090\n",
      "Epoch 99/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8842 - val_loss: 0.5556 - val_accuracy: 0.8090\n",
      "Epoch 100/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3340 - accuracy: 0.8836 - val_loss: 0.5537 - val_accuracy: 0.8087\n",
      "Epoch 101/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3328 - accuracy: 0.8847 - val_loss: 0.5523 - val_accuracy: 0.8067\n",
      "Epoch 102/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3316 - accuracy: 0.8848 - val_loss: 0.5538 - val_accuracy: 0.8070\n",
      "Epoch 103/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.8851 - val_loss: 0.5554 - val_accuracy: 0.8113\n",
      "Epoch 104/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8861 - val_loss: 0.5560 - val_accuracy: 0.8070\n",
      "Epoch 105/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3277 - accuracy: 0.8866 - val_loss: 0.5567 - val_accuracy: 0.8077\n",
      "Epoch 106/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8864 - val_loss: 0.5582 - val_accuracy: 0.8060\n",
      "Epoch 107/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8868 - val_loss: 0.5606 - val_accuracy: 0.8073\n",
      "Epoch 108/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3238 - accuracy: 0.8887 - val_loss: 0.5639 - val_accuracy: 0.8047\n",
      "Epoch 109/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8881 - val_loss: 0.5621 - val_accuracy: 0.8077\n",
      "Epoch 110/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3212 - accuracy: 0.8896 - val_loss: 0.5613 - val_accuracy: 0.8060\n",
      "Epoch 111/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8888 - val_loss: 0.5624 - val_accuracy: 0.8047\n",
      "Epoch 112/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3192 - accuracy: 0.8894 - val_loss: 0.5656 - val_accuracy: 0.8100\n",
      "Epoch 113/120\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 0.3175 - accuracy: 0.8905 - val_loss: 0.5641 - val_accuracy: 0.8070\n",
      "Epoch 114/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3165 - accuracy: 0.8915 - val_loss: 0.5661 - val_accuracy: 0.8080\n",
      "Epoch 115/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3152 - accuracy: 0.8918 - val_loss: 0.5681 - val_accuracy: 0.8070\n",
      "Epoch 116/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.8911 - val_loss: 0.5690 - val_accuracy: 0.8057\n",
      "Epoch 117/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3129 - accuracy: 0.8923 - val_loss: 0.5710 - val_accuracy: 0.8077\n",
      "Epoch 118/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8922 - val_loss: 0.5691 - val_accuracy: 0.8063\n",
      "Epoch 119/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8935 - val_loss: 0.5750 - val_accuracy: 0.8073\n",
      "Epoch 120/120\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.3093 - accuracy: 0.8930 - val_loss: 0.5730 - val_accuracy: 0.8073\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, let's check the results returned from `model.evaluate()` to see how this model stacks up with the other techniques we've used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032/1032 [==============================] - 0s 473us/step - loss: 0.3046 - accuracy: 0.8963\n",
      "125/125 [==============================] - 0s 664us/step - loss: 0.5692 - accuracy: 0.7995\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3046054244041443, 0.8963030576705933]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train # Expected Output:  [0.31160746300942971, 0.89160606060606062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5692464113235474, 0.7994999885559082]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test # Expected Output: [0.56076071488857271, 0.8145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.1%. Our test set accuracy went up from ~75% to a staggering 81.45% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
